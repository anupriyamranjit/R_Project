---
title: "Fraud Classification"
output:
  pdf_document: default
  html_document: default
---

## Business Question
Can we predict whether a transaction is fraud?

## Setup and Initialization

```{r}
# Setup
knitr::opts_chunk$set(echo = TRUE)
```

### Data Import
```{r}
# Import Data
data_original <- read.csv('./Fraud.csv')

```

```{r}
# Copy Data
data_copy <- data_original

```

### Data Cleaning and Preprocessing
```{r}
# See Structure of Data and Summary for any issues 
str(data_copy)
summary(data_copy)

```

```{r}
# Check Number of NA values in data
sum(is.na(data_copy))

```

```{r}
# Preprocess Data 
data_copy$merchant <- ifelse(substr(data_copy$nameDest, 1, 1) == 'M', TRUE, FALSE)
data_copy$isFraud <- as.logical(data_copy$isFraud)
data_copy$isFlaggedFraud <- NULL
data_copy$nameOrig <- NULL
data_copy$nameDest <- NULL
data_copy$type <- as.integer(as.factor(data_copy$type))
summary(data_copy)

```

### Training and Testing Sets Creation
```{r}
# Create Training and Testing Sets
set.seed(10)
smp_size <- floor(0.75 * nrow(data_copy))
print(paste("Sample size: ", smp_size))

set.seed(100)
train_ind <- sample(1:nrow(data_copy), size = smp_size)

train <- data_copy[train_ind, ]
test <- data_copy[-train_ind, ]

```

## Avoiding Complexity: Feature (Model) Selection
We want to avoid complexity by tuning the classification model complexity to the classification task. We want to ensure there are not too many features which can lead to overfitting while too few and the model can not learn good rules. The Regularization method will be used here with a Lasso (L1) penalty function. 

This is because L1 tends to shrink coefficients to zero whereas Ridge (L2) tends to shrink coefficients evenly. L1 is therefore useful for feature selection, as we can drop any variables associated with coefficients that go to zero.

Ref: https://explained.ai/regularization/L1vsL2.html \
Ref: https://www.statology.org/lasso-regression-in-r/ \

First, we need to define the response variable and all the features as a matrix
```{r}
# lets define the response variable
y <- train$isFraud

# lets define a matrix of all features except for the columns related to the response variable
all_column_names <- colnames(train)
all_columns_noFraud <- all_column_names[all_column_names != "isFraud"]
x <- data.matrix(train[, all_columns_noFraud])
```

Then, we need to determine the optimal lambda value to use which will be done using cross-validation (CV)
```{r}
library(glmnet)
library(Matrix)
library(doParallel)


if (file.exists("./cv_model.RData")) {
  # load cv_model
  load("./cv_model.RData")
  
} else {
  # create cv_model
  
  num_cores <- 4
  cl <- makeCluster(num_cores)
  registerDoParallel(cl)
  
  # perform k-fold cross-validation to find optimal lambda value (using the default k=10 folds)
  # use alpha=1 as we want to fit the lasso regression model
  cv_model <- cv.glmnet(x, y, alpha = 1, parallel = TRUE)
  
  stopCluster(cl)
  
  # save cv_model
  save(cv_model, file="./cv_model.RData")

}

# determine optimal lambda value that minimizes test mean squared error (MSE)
optimal_lambda <- cv_model$lambda.min
print("The Optimal Lambda is:")
optimal_lambda

# produce plot of test mean squared error (MSE) by lambda value
plot(cv_model)

```

Finally, we can use the optimal lambda value to determine coefficient estimates for each variable
```{r}


if (file.exists("./regularized_model.RData")) {
  # load regularized_model
  load("./regularized_model.RData")
  
} else {
  # create regularized_model
  
  num_cores <- 4
  cl <- makeCluster(num_cores)
  registerDoParallel(cl)

  
  # determine coefficient estimates for each variable of the regularized model
  regularized_model <- glmnet(x, y, alpha = 1, lambda = optimal_lambda)
  
  stopCluster(cl)
  
  # save regularized_model
  save(regularized_model, file="./regularized_model.RData")

}

# create coefficient matrix
coef_matrix <- coef(regularized_model)
coef_matrix
```

Let's construct a formula with only the significant features.
```{r}
# drop the Intercept column as it is not needed
row_index <- which(rownames(coef_matrix) == "(Intercept)")
coef_matrix_noIntercept <- coef_matrix[-row_index, , drop = FALSE]

# store only the feature names that don't have a coefficient of 0
ideal_features <- rownames(coef_matrix_noIntercept)[coef_matrix_noIntercept[, 1] != 0][]
ideal_features

# construct the formula as a string
formula_string <- paste("isFraud ~", paste(ideal_features, collapse = " + "))
formula_string

# convert the formula string to a formula object
formula_object <- as.formula(formula_string)
print("Final relation:")
print(formula_object)

```

## Model 1: Logistic Regression

For the first model, we generate a logistic regression classifier for the formula defined above. We train the model on the previously defined training set and evaluate this model using the testing set. We use a cut value of 0.5 since this is a widely accepted value and fits our case.
```{r}
if (file.exists("./logReg.RData")) {
  # load classifier
  load("./logReg.RData")
} else {
  # create classifier
  cls <- glm(formula_object, family='binomial',data=train)
  
  # save classifier
  save(cls, file="./logReg.RData")
}
```

### Training and Testing Error Assessment

Now, let's evaluate the training and testing error for the Logistic Regression Classifier
```{r}
cut=0.5

yhat_tr = (predict(cls,train,type="response")>cut)
tr.err = mean(train$isFraud != yhat_tr) 

yhat_te = (predict(cls,test,type="response")>cut)
te.err = mean(test$isFraud != yhat_te) 

tr.err
te.err
```
In this case, we see a very low error rate for the training set, which means the model is not underfit. 
In addition, the testing error is very similar to the training error, meaning that the model is not overfit.

## Model 2: Decision Tree Classifier Model

### Classifier Generation

We can generate a decision tree classifier trained on the training set and evaluated on the testing set. Since complexity parameter (cp) controls the improvement threshold to make a split, we can compare different thresholds.
```{r}
# define and train each of the classifiers
library(rpart)

```

```{r}
if (file.exists("./tree1.RData")) {
  # load tree1
  load("./tree1.RData")
  
} else {
  # create tree1
  tree1 <- rpart(formula_object,method="class", data=train, cp=0.01)
  
  # save tree1
  save(tree1, file="./tree1.RData")

}


```


``` {r}
if (file.exists("./tree2.RData")) {
  # load tree2
  load("./tree2.RData")
  
} else {
  # create tree2
  tree2 <- rpart(formula_object,method="class", data=train, cp=0.001)
  
  # save tree2
  save(tree2, file="./tree2.RData")

}

```


```{r}
if (file.exists("./tree3.RData")) {
  # load tree3
  load("./tree3.RData")
  
} else {
  # create tree3
  tree3 <- rpart(formula_object,method="class", data=train, cp=0.00001)
  
  # save tree3
  save(tree3, file="./tree3.RData")

}

```


### Training and Testing Error Assessment

Now, lets compare the training and testing error for cp=0.01, cp=0.001, and cp=0.00001
```{r}
treeErr <- function(tree, test_dataset, train_dataset)
{
  test_pred = predict(tree,test_dataset,type="class")
  test_err = mean(test_dataset$isFraud != test_pred)
  
  
  train_pred = predict(tree,train_dataset,type="class")
  train_err = mean(train_dataset$isFraud != train_pred)
  
  test_acc = 1 - test_err
  
  conf_matrix <- table(Actual = test_dataset$isFraud, Predicted = test_pred)
  
  result_df <- data.frame(
    Metric = c("Train Error", "Test Error", "Accuracy"),
    Value = c(train_err, test_err, test_acc)
  )
  
  return(list(result_df, Confusion_Matrix = conf_matrix))
  
}
```


```{r}
# Errors for tree1
tree1Err <- treeErr(tree1, test, train)
print(tree1Err)

```

```{r}
# Errors for tree2
tree2Err <- treeErr(tree2, test, train)
print(tree2Err)

```

```{r}
# Errors for tree3
tree3Err <- treeErr(tree3, test, train)
print(tree3Err)

```

It is clear that for cp=0.01 the tree is underfit as it does not split and defaults to classifying all transactions as fraud. For cp=0.001 the training error and testing error improve but only slightly. For cp=0.000001, the tree is much more complex and we see evidence of overfitting.

## Model 3: Support Vector Machine (SVM) Model

Since SVM takes a very long time to run for large dataset, we will use the data reduction strategy to randomly select a subset of our data for training and testing
```{r}
library(e1071)

# Training / testing data for SVM
set.seed(100) # Ensure reproducibility
sample_size <- floor(0.1 * nrow(data_copy)) # Calculate 10% of the data size
sample_indices <- sample(1:nrow(data_copy), size = sample_size) # Get random sample indices
data_subset <- data_copy[sample_indices, ] # Extract 10% of the data

set.seed(100)
train_size <- floor(0.7 * nrow(data_subset)) # Calculate 70% of the subset size for training
train_indices <- sample(1:nrow(data_subset), size = train_size) # Get random sample indices for training set

train_svm <- data_subset[train_indices, ] # Create the training set from the subset
test_svm <- data_subset[-train_indices, ] # Create the testing set from the subset


train_svm$isFraud <- as.factor(train_svm$isFraud) #The target variable should be a factor
test_svm$isFraud <- as.factor(test_svm$isFraud)

if (file.exists("./svm.RData")) {
  # load svm
  load("./svm.RData")
  
} else {
  # create svm
  svm_model <- svm(formula = formula_object, 
                 data = train_svm, 
                 kernel = "radial", 
                 cost = 1,
                 scale = TRUE) # Automatic feature scaling
  
  # save svm
  save(svm_model, file="./svm.RData")

}

library(caret)

# Predictions on the training set
train_predictions <- predict(svm_model, newdata = train_svm)

# Predictions on the testing set
test_predictions <- predict(svm_model, newdata = test_svm)

# Calculating Training Error
training_error <- mean(train_predictions != train_svm$isFraud)
print(paste("Training Error: ", training_error))

# Calculating Testing Error
testing_error <- mean(test_predictions != test_svm$isFraud)
print(paste("Testing Error: ", testing_error))

# Confusion Matrix
confusionMatrix(predictions, test_svm$isFraud)

```
## Model 4: K-Nearest Neighbours (KNN) Model

```{r}

#KNN

library(FNN)  # Faster k-nearest neighbor algorithm implementation
library(doParallel)  # For parallel computation if needed

# Example parallelization setup
# Adjust the number of cores as per your machine's configuration
cores <- 4
registerDoParallel(cores=cores)

class_labels <- train$isFraud  # Assuming "isFraud" is the target variable
train_features <- train[, -which(names(train) == "isFraud")]  # Exclude the target variab
test_features <- test[, -which(names(test) == "isFraud")]  # Exclude the target variab

# Example of using FNN library which might be faster
test_pred <- knn(train_features, test = test_features, cl = class_labels, k = 10)

# Stop parallel computation
stopImplicitCluster()



```

```{r}
actual <- test$isFraud
cm <- table(actual,test_pred)
cm
```
```{r}
accuracy <- sum(diag(cm))/length(actual)
sprintf("KNN Accuracy: %.10f%%", accuracy*100)
```


## Model 5: Neural Net (NN) Model

```{r}
library(tidyverse)
library(keras)
library(tensorflow)

```

```{r}


checkpoint_path <- "training_2/cp.ckpt"
checkpoint_dir <- fs::path_dir(checkpoint_path)


# Define predictor variables
predictor_vars <- c("step", "amount", "oldbalanceOrg", "newbalanceOrig", "oldbalanceDest", "newbalanceDest", "type", "merchant")
formula <- as.formula(paste("isFraud ~", paste(predictor_vars, collapse = "+")))


if (file.exists("new_model.hdf5")) {
  
  model <- load_model_hdf5("new_model.hdf5")
  message("Loaded weights from checkpoint.")
  
} else {
  
  model <- keras_model_sequential() %>%
  layer_dense(units = 4, activation = "relu", input_shape = length(predictor_vars)) %>%
  layer_dense(units = 2, activation = "relu") %>%
  layer_dense(units = 1, activation = "sigmoid")


# Compile the model
model %>% compile(
  loss = "binary_crossentropy",
  optimizer =  tf$keras$optimizers$legacy$Adam(),
  metrics = "accuracy"
)


cp_callback <- callback_model_checkpoint(
  filepath = checkpoint_path,
  save_weights_only = TRUE,
  verbose = 1
)


# Train the model
history <- model %>% fit(
  x = as.matrix(train[predictor_vars]),
  y = train$isFraud,
  epochs = 10,
  batch_size = 100,
  class_weights= c(1,1000),
  validation_split = 0.3,  # Split data for validation
  callbacks = list(cp_callback, early_stopping) # Pass callback to training
)

model %>% save_model_hdf5("./new_model.hdf5")
  
}

# Evaluate the model
model %>% evaluate(
  x = as.matrix(test[predictor_vars]),
  y = test$isFraud
)

```


```{r}
predicted_probs <- model %>% predict(as.matrix(test[predictor_vars]))
predicted_classes <- ifelse(predicted_probs > 0.5, TRUE, FALSE)
results <- data.frame(Actual = test$isFraud, Predicted = predicted_classes)
conf_matrix <- table(Actual = results$Actual, Predicted = results$Predicted)
print(conf_matrix)
```

